{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_pdf_text(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text from PDF: {str(e)}\"\n",
    "\n",
    "import nbformat\n",
    "\n",
    "def extract_ipynb_text(ipynb_path):\n",
    "    try:\n",
    "        with open(ipynb_path, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbformat.read(f, as_version=4)\n",
    "            text = \"\"\n",
    "            for cell in notebook.cells:\n",
    "                if cell.cell_type == 'code' or cell.cell_type == 'markdown':\n",
    "                    text += ''.join(cell['source'])\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text from IPYNB: {str(e)}\"\n",
    "    \n",
    "def extract_py_text(py_path):\n",
    "    try:\n",
    "        with open(py_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text from PY: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   0%|          | 0/283 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No text extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   0%|          | 1/283 [00:02<13:49,  2.94s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "# 加载数据\n",
      "df = pd.read_csv('train.csv')\n",
      "\n",
      "# 查看数据前几行\n",
      "print(df.head())\n",
      "# 选择特征和标签\n",
      "features = df.drop('Transported', axis=1)\n",
      "labels = df['Transported']\n",
      "\n",
      "# 分类和数值特征的列名\n",
      "categorical_features = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\n",
      "numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
      "\n",
      "# 数据预处理\n",
      "# 为数值特征和分类特征创建预处理步骤\n",
      "numerical_pipeline = Pipeline(steps=[\n",
      "    ('imputer', SimpleImputer(strategy='median')),\n",
      "    ('scaler', StandardScaler())\n",
      "])\n",
      "\n",
      "categorical_pipeline = Pipeline(steps=[\n",
      "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
      "])\n",
      "\n",
      "# 合并这些步骤\n",
      "preprocessor = ColumnTransformer(transformers=[\n",
      "    ('num', numerical_pipeline, numerical_features),\n",
      "    ('cat', categorical_pipeline, categorical_features)\n",
      "])\n",
      "\n",
      "# 应用预处理\n",
      "X_preprocessed = preprocessor.fit_transform(features)\n",
      "# 分割数据集\n",
      "X_train, X_val, y_train, y_val = train_test_split(X_preprocessed, labels, test_size=0.2, random_state=42)\n",
      "\n",
      "# 因为预处理后的X是numpy数组，我们可以直接查看它们的shape\n",
      "print(\"Training set shape:\", X_train.shape)\n",
      "print(\"Validation set shape:\", X_val.shape)\n",
      "import numpy as np\n",
      "from datetime import datetime \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "\n",
      "from torchvision import datasets, transforms\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "# device = torch.device(\"cpu\")def train(train_loader, model, criterion, optimizer):\n",
      "    model.train()\n",
      "    running_loss = 0\n",
      "    \n",
      "    for X, y_true in train_loader:\n",
      "        X, y_true = X.to(device), y_true.to(device)\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "    \n",
      "        # Forward pass\n",
      "        y_hat = model(X) \n",
      "        loss = criterion(y_hat, y_true)\n",
      "        running_loss += loss.item() * X.size(0)\n",
      "\n",
      "        # Backward pass\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        \n",
      "    epoch_loss = running_loss / len(train_loader.dataset)\n",
      "    return model, optimizer, epoch_lossdef validate(valid_loader, model, criterion):\n",
      "    model.eval()\n",
      "    running_loss = 0\n",
      "    for X, y_true in valid_loader:\n",
      "        X, y_true = X.to(device), y_true.to(device)\n",
      "        y_hat = model(X)\n",
      "        loss = criterion(y_hat, y_true)\n",
      "        running_loss += loss.item() * X.size(0)\n",
      "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
      "    return model, epoch_lossdef get_accuracy(model, data_loader):\n",
      "    model.eval()\n",
      "    correct_pred = 0 \n",
      "    n = 0\n",
      "    with torch.no_grad():\n",
      "        for X, y_true in data_loader:\n",
      "            X, y_true = X.to(device), y_true.to(device)\n",
      "            y_hat = model(X)\n",
      "            _, predicted_labels = torch.max(y_hat, 1)\n",
      "            n += y_true.size(0)\n",
      "            correct_pred += (predicted_labels == y_true).sum()\n",
      "    accuracy = correct_pred.float() / n\n",
      "    return accuracy.item()def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, print_every=1):\n",
      "    train_losses = []\n",
      "    valid_losses = []\n",
      "    train_accs = []\n",
      "    valid_accs = []\n",
      "    for epoch in range(0, epochs):\n",
      "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer)\n",
      "        train_losses.append(train_loss)\n",
      "        with torch.no_grad():\n",
      "            model, valid_loss = validate(valid_loader, model, criterion)\n",
      "            valid_losses.append(valid_loss)\n",
      "        if epoch % print_every == (print_every - 1):\n",
      "            train_acc = get_accuracy(model, train_loader,)\n",
      "            train_accs.append(train_acc)\n",
      "            valid_acc = get_accuracy(model, valid_loader)\n",
      "            valid_accs.append(valid_acc)\n",
      "            print(f'{datetime.now().time().replace(microsecond=0)} '\n",
      "                  f'Epoch: {epoch}\\t'\n",
      "                  f'Train loss: {train_loss:.4f}\\t'\n",
      "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
      "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
      "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
      "    performance = {\n",
      "        'train_losses':train_losses,\n",
      "        'valid_losses': valid_losses,\n",
      "        'train_acc': train_accs,\n",
      "        'valid_acc':valid_accs\n",
      "    }\n",
      "    return model, optimizer, performance### 动态模型构造class SpaceshipTitanicModel(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, layers):\n",
      "        super(SpaceshipTitanicModel, self).__init__()\n",
      "        self.layers = nn.ModuleList()\n",
      "        \n",
      "        # 添加第一个隐藏层，它接受输入数据\n",
      "        self.layers.append(nn.Linear(input_dim, layers[0]))\n",
      "        self.layers.append(nn.ReLU())\n",
      "        \n",
      "        # 动态添加更多的隐藏层\n",
      "        for i in range(1, len(layers)):\n",
      "            self.layers.append(nn.Linear(layers[i-1], layers[i]))\n",
      "            self.layers.append(nn.ReLU())\n",
      "        \n",
      "        # 输出层\n",
      "        self.layers.append(nn.Linear(layers[-1], output_dim))\n",
      "        \n",
      "    def forward(self, x):\n",
      "        for layer in self.layers:\n",
      "            x = layer(x)\n",
      "        return x\n",
      "### 数据加载器处理from torch.utils.data import TensorDataset, DataLoader\n",
      "\n",
      "BATCH_SIZE = 30\n",
      "\n",
      "\n",
      "# 将预处理后的数据从稀疏矩阵转换为密集的NumPy数组\n",
      "X_train_dense = X_train.toarray() if hasattr(X_train, \"toarray\") else X_train\n",
      "X_val_dense = X_val.toarray() if hasattr(X_val, \"toarray\") else X_val\n",
      "\n",
      "# 将NumPy数组转换为PyTorch张量\n",
      "X_train_tensor = torch.tensor(X_train_dense.astype(np.float32))\n",
      "y_train_tensor = torch.tensor(y_train.values.astype(np.int64))\n",
      "X_val_tensor = torch.tensor(X_val_dense.astype(np.float32))\n",
      "y_val_tensor = torch.tensor(y_val.values.astype(np.int64))\n",
      "\n",
      "# 创建TensorDataset对象\n",
      "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
      "valid_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
      "\n",
      "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
      "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)### 首个模型 input size -> 648 -> 328 -> 128 -> 2# 根据预处理后的数据维度初始化模型\n",
      "# 这里是一开始提出的模型\n",
      "LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 20\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [648, 328, 128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "print(model)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "\n",
      "\n",
      "# model, optimizer, performance = training_loop(model, criterion, optimizer,train_loader,valid_loader, N_EPOCHS)\n",
      "### K折验证from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "# 定义K折交叉验证\n",
      "n_splits = 5\n",
      "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
      "performance = {'train_losses': [], 'valid_losses': [], 'train_acc': [], 'valid_acc': []}\n",
      "# performance = {\n",
      "#         'train_losses':train_losses,\n",
      "#         'valid_losses': valid_losses,\n",
      "#         'train_acc': train_accs,\n",
      "#         'valid_acc':valid_accs\n",
      "#     }\n",
      "for fold, (train_idx, val_idx) in enumerate(skf.split(X_preprocessed, labels)):\n",
      "    print(f'Fold {fold + 1}/{n_splits}')\n",
      "    \n",
      "    # 分割数据\n",
      "    X_train_fold, y_train_fold = X_preprocessed[train_idx], labels.iloc[train_idx]\n",
      "    X_val_fold, y_val_fold = X_preprocessed[val_idx], labels.iloc[val_idx]\n",
      "    \n",
      "\n",
      "    # 转换为PyTorch数据类型，确保先转换为密集格式\n",
      "    X_train_fold_dense = X_train_fold.toarray() if hasattr(X_train_fold, \"toarray\") else X_train_fold\n",
      "    X_val_fold_dense = X_val_fold.toarray() if hasattr(X_val_fold, \"toarray\") else X_val_fold\n",
      "\n",
      "    # 转换为PyTorch数据类型\n",
      "    X_train_fold_tensor = torch.tensor(X_train_fold_dense.astype(np.float32)).to(device)\n",
      "    y_train_fold_tensor = torch.tensor(y_train_fold.values.astype(np.int64)).to(device)\n",
      "    X_val_fold_tensor = torch.tensor(X_val_fold_dense.astype(np.float32)).to(device)\n",
      "    y_val_fold_tensor = torch.tensor(y_val_fold.values.astype(np.int64)).to(device)\n",
      "    \n",
      "    # DataLoader\n",
      "    train_dataset = TensorDataset(X_train_fold_tensor, y_train_fold_tensor)\n",
      "    val_dataset = TensorDataset(X_val_fold_tensor, y_val_fold_tensor)\n",
      "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
      "    valid_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
      "    \n",
      "    # 初始化模型、损失函数、优化器\n",
      "    model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "    \n",
      "    # 训练和验证\n",
      "    model, optimizer, fold_performance = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS)\n",
      "    \n",
      "    # 记录性能\n",
      "    for key in performance.keys():\n",
      "        performance[key].append(fold_performance[key])\n",
      "for key in performance.keys():\n",
      "    print(f'{key}: {np.mean(performance[key])} ± {np.std(performance[key])}')\n",
      "提交后，排名分数为0.76174\n",
      "### 新模型提出 + K 折验证因为k折训练出来的准确率太差，所以我打算先从3层神经网络开始。第一层为features的总数，第二层为features总数的一半LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 10\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4]  # 可以调整这些层的大小和数量\n",
      "\n",
      "model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "print(model)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "# 定义K折交叉验证\n",
      "n_splits = 5\n",
      "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
      "performance = {'train_losses': [], 'valid_losses': [], 'train_acc': [], 'valid_acc': []}\n",
      "# performance = {\n",
      "#         'train_losses':train_losses,\n",
      "#         'valid_losses': valid_losses,\n",
      "#         'train_acc': train_accs,\n",
      "#         'valid_acc':valid_accs\n",
      "#     }\n",
      "for fold, (train_idx, val_idx) in enumerate(skf.split(X_preprocessed, labels)):\n",
      "    print(f'Fold {fold + 1}/{n_splits}')\n",
      "    \n",
      "    # 分割数据\n",
      "    X_train_fold, y_train_fold = X_preprocessed[train_idx], labels.iloc[train_idx]\n",
      "    X_val_fold, y_val_fold = X_preprocessed[val_idx], labels.iloc[val_idx]\n",
      "    \n",
      "\n",
      "    # 转换为PyTorch数据类型，确保先转换为密集格式\n",
      "    X_train_fold_dense = X_train_fold.toarray() if hasattr(X_train_fold, \"toarray\") else X_train_fold\n",
      "    X_val_fold_dense = X_val_fold.toarray() if hasattr(X_val_fold, \"toarray\") else X_val_fold\n",
      "\n",
      "    # 转换为PyTorch数据类型\n",
      "    X_train_fold_tensor = torch.tensor(X_train_fold_dense.astype(np.float32)).to(device)\n",
      "    y_train_fold_tensor = torch.tensor(y_train_fold.values.astype(np.int64)).to(device)\n",
      "    X_val_fold_tensor = torch.tensor(X_val_fold_dense.astype(np.float32)).to(device)\n",
      "    y_val_fold_tensor = torch.tensor(y_val_fold.values.astype(np.int64)).to(device)\n",
      "    \n",
      "    # DataLoader\n",
      "    train_dataset = TensorDataset(X_train_fold_tensor, y_train_fold_tensor)\n",
      "    val_dataset = TensorDataset(X_val_fold_tensor, y_val_fold_tensor)\n",
      "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
      "    valid_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
      "    \n",
      "    # 初始化模型、损失函数、优化器\n",
      "    model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "    \n",
      "    # 训练和验证\n",
      "    model, optimizer, fold_performance = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS)\n",
      "    \n",
      "    # 记录性能\n",
      "    for key in performance.keys():\n",
      "        performance[key].append(fold_performance[key])\n",
      "for key in performance.keys():\n",
      "    print(f'{key}: {np.mean(performance[key])} ± {np.std(performance[key])}')### 第三次模型提出我发现首个epoch的准确率提高了很多，然后后面逐步下降到比一开始模型还要低的Valid accuracy。所以我打算直接接入原本的模型LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 10\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4,648,328,128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "print(model)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "# 定义K折交叉验证\n",
      "n_splits = 5\n",
      "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
      "performance = {'train_losses': [], 'valid_losses': [], 'train_acc': [], 'valid_acc': []}\n",
      "# performance = {\n",
      "#         'train_losses':train_losses,\n",
      "#         'valid_losses': valid_losses,\n",
      "#         'train_acc': train_accs,\n",
      "#         'valid_acc':valid_accs\n",
      "#     }\n",
      "for fold, (train_idx, val_idx) in enumerate(skf.split(X_preprocessed, labels)):\n",
      "    print(f'Fold {fold + 1}/{n_splits}')\n",
      "    \n",
      "    # 分割数据\n",
      "    X_train_fold, y_train_fold = X_preprocessed[train_idx], labels.iloc[train_idx]\n",
      "    X_val_fold, y_val_fold = X_preprocessed[val_idx], labels.iloc[val_idx]\n",
      "    \n",
      "\n",
      "    # 转换为PyTorch数据类型，确保先转换为密集格式\n",
      "    X_train_fold_dense = X_train_fold.toarray() if hasattr(X_train_fold, \"toarray\") else X_train_fold\n",
      "    X_val_fold_dense = X_val_fold.toarray() if hasattr(X_val_fold, \"toarray\") else X_val_fold\n",
      "\n",
      "    # 转换为PyTorch数据类型\n",
      "    X_train_fold_tensor = torch.tensor(X_train_fold_dense.astype(np.float32)).to(device)\n",
      "    y_train_fold_tensor = torch.tensor(y_train_fold.values.astype(np.int64)).to(device)\n",
      "    X_val_fold_tensor = torch.tensor(X_val_fold_dense.astype(np.float32)).to(device)\n",
      "    y_val_fold_tensor = torch.tensor(y_val_fold.values.astype(np.int64)).to(device)\n",
      "    \n",
      "    # DataLoader\n",
      "    train_dataset = TensorDataset(X_train_fold_tensor, y_train_fold_tensor)\n",
      "    val_dataset = TensorDataset(X_val_fold_tensor, y_val_fold_tensor)\n",
      "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
      "    valid_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
      "    \n",
      "    # 初始化模型、损失函数、优化器\n",
      "    model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "    \n",
      "    # 训练和验证\n",
      "    model, optimizer, fold_performance = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS)\n",
      "    \n",
      "    # 记录性能\n",
      "    for key in performance.keys():\n",
      "        performance[key].append(fold_performance[key])\n",
      "for key in performance.keys():\n",
      "    print(f'{key}: {np.mean(performance[key])} ± {np.std(performance[key])}')虽然训练准确率有所下降，但是验证集准确率达到了和第一个模型几乎一致的准去率。我有觉得这可能代表了这个train data的准确率最高应该就在73.8%左右\n",
      "因为这个模型的训练准确率反而有所下降，在验证准确率不变的情况下，我有理由认为这个模型在可靠的情况下(95%+准确率)会有更好的泛化效果。### 利用所有的资源训练模型我首先选择30次 0.00的LR 先看看结果如何# 根据预处理后的数据维度初始化模型\n",
      "LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 30\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4,648,328,128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "\n",
      "model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "print(model)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "\n",
      "\n",
      "model, optimizer, performance = training_loop(model, criterion, optimizer,train_loader,valid_loader, N_EPOCHS)似乎有3次重大的下跌，分别是27次，20次，17次，6次。我之前做k折验证的时候也发现似乎在第六次的时候会有一次断崖式下跌。在折30次的训练中，17~20次之间训练和验证准确率都很高。但是范围太小了，以后训练中难以取舍。反倒是6次到17次之间，有着高训练准确率的同时，验证准确率也不错。那么我则选择其平均值 (17-6)/2 + 6 = 11.5 我选择向上取整，选择12次。同时，改变我的LR。我决定每5次训练减少我的LR以达到更接近12次的效果。所以接下来的策略应该是30次训练，每5次学习率减少一半。这样既可以让准确率增加，也可以避免循环次数带来的断崖下跌问题##### 定时更新LRfrom torch.optim import lr_scheduler\n",
      "# 根据预处理后的数据维度初始化模型\n",
      "LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 30\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4,648,328,128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "\n",
      "model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "print(model)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
      "\n",
      "model, optimizer, performance = training_loop(model, criterion, optimizer,train_loader,valid_loader, N_EPOCHS)25次后出现了断崖式下跌，10~14次，17~19次都是较为准确的。这次的最终训练结果都很高。再一次提交模型看看这次成绩排名为2011，分数为0.76969。比较上次有提升但是并不显著由于发现几乎15次训练以后有断崖式准度下跌，所以训练次数改为15次，并且更频繁的更新LRfrom torch.optim import lr_scheduler\n",
      "# 根据预处理后的数据维度初始化模型\n",
      "LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 15\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4,648,328,128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "\n",
      "model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "print(model)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
      "\n",
      "model, optimizer, performance = training_loop(model, criterion, optimizer,train_loader,valid_loader, N_EPOCHS)这个方案发现验证集反而更差了。所以这个方案不是一个有效的方案。应该从验证集的准确率开始下手，降低tran loss的同时也要降低valid lossdef training_loop_updateValidLoss(model, criterion, optimizer, train_loader, valid_loader, epochs, print_every=1):\n",
      "    train_losses = []\n",
      "    valid_losses = []\n",
      "    train_accs = []\n",
      "    valid_accs = []\n",
      "    for epoch in range(0, epochs):\n",
      "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer)\n",
      "        train_losses.append(train_loss)\n",
      "        with torch.no_grad():\n",
      "            model, valid_loss = validate(valid_loader, model, criterion)\n",
      "            valid_losses.append(valid_loss)\n",
      "        if epoch % print_every == (print_every - 1):\n",
      "            train_acc = get_accuracy(model, train_loader,)\n",
      "            train_accs.append(train_acc)\n",
      "            valid_acc = get_accuracy(model, valid_loader)\n",
      "            valid_accs.append(valid_acc)\n",
      "            print(f'{datetime.now().time().replace(microsecond=0)} '\n",
      "                  f'Epoch: {epoch}\\t'\n",
      "                  f'Train loss: {train_loss:.4f}\\t'\n",
      "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
      "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
      "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
      "            scheduler.step(valid_loss)\n",
      "    performance = {\n",
      "        'train_losses':train_losses,\n",
      "        'valid_losses': valid_losses,\n",
      "        'train_acc': train_accs,\n",
      "        'valid_acc':valid_accs\n",
      "    }\n",
      "    return model, optimizer, performanceimport torch.optim as optim\n",
      "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
      "# 根据预处理后的数据维度初始化模型\n",
      "LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 30\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4,648,328,128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "\n",
      "model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "print(model)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=10, verbose=True)\n",
      "\n",
      "model, optimizer, performance = training_loop_updateValidLoss(model, criterion, optimizer,train_loader,valid_loader, N_EPOCHS)### 进一步修改模型在各种尝试不佳以后我认为应该从根本上修改模型。首先尝试正则化class SpaceshipTitanicModelDropOut(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, layers, dropout_rate=0.5):\n",
      "        super(SpaceshipTitanicModelDropOut, self).__init__()\n",
      "        self.layers = nn.ModuleList()\n",
      "        \n",
      "        # 添加第一个隐藏层，它接受输入数据\n",
      "        self.layers.append(nn.Linear(input_dim, layers[0]))\n",
      "        self.layers.append(nn.ReLU())\n",
      "        self.layers.append(nn.Dropout(dropout_rate))\n",
      "        \n",
      "        # 动态添加更多的隐藏层\n",
      "        for i in range(1, len(layers)):\n",
      "            self.layers.append(nn.Linear(layers[i-1], layers[i]))\n",
      "            self.layers.append(nn.ReLU())\n",
      "            self.layers.append(nn.Dropout(dropout_rate))\n",
      "        \n",
      "        # 输出层\n",
      "        self.layers.append(nn.Linear(layers[-1], output_dim))\n",
      "        \n",
      "    def forward(self, x):\n",
      "        for layer in self.layers:\n",
      "            x = layer(x)\n",
      "        return x\n",
      "LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 10\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [648, 328, 128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "model = SpaceshipTitanicModelDropOut(input_dim, output_dim, hidden_layers).to(device)\n",
      "print(model)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "# 定义K折交叉验证\n",
      "n_splits = 5\n",
      "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
      "performance = {'train_losses': [], 'valid_losses': [], 'train_acc': [], 'valid_acc': []}\n",
      "# performance = {\n",
      "#         'train_losses':train_losses,\n",
      "#         'valid_losses': valid_losses,\n",
      "#         'train_acc': train_accs,\n",
      "#         'valid_acc':valid_accs\n",
      "#     }\n",
      "for fold, (train_idx, val_idx) in enumerate(skf.split(X_preprocessed, labels)):\n",
      "    print(f'Fold {fold + 1}/{n_splits}')\n",
      "    \n",
      "    # 分割数据\n",
      "    X_train_fold, y_train_fold = X_preprocessed[train_idx], labels.iloc[train_idx]\n",
      "    X_val_fold, y_val_fold = X_preprocessed[val_idx], labels.iloc[val_idx]\n",
      "    \n",
      "\n",
      "    # 转换为PyTorch数据类型，确保先转换为密集格式\n",
      "    X_train_fold_dense = X_train_fold.toarray() if hasattr(X_train_fold, \"toarray\") else X_train_fold\n",
      "    X_val_fold_dense = X_val_fold.toarray() if hasattr(X_val_fold, \"toarray\") else X_val_fold\n",
      "\n",
      "    # 转换为PyTorch数据类型\n",
      "    X_train_fold_tensor = torch.tensor(X_train_fold_dense.astype(np.float32)).to(device)\n",
      "    y_train_fold_tensor = torch.tensor(y_train_fold.values.astype(np.int64)).to(device)\n",
      "    X_val_fold_tensor = torch.tensor(X_val_fold_dense.astype(np.float32)).to(device)\n",
      "    y_val_fold_tensor = torch.tensor(y_val_fold.values.astype(np.int64)).to(device)\n",
      "    \n",
      "    # DataLoader\n",
      "    train_dataset = TensorDataset(X_train_fold_tensor, y_train_fold_tensor)\n",
      "    val_dataset = TensorDataset(X_val_fold_tensor, y_val_fold_tensor)\n",
      "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
      "    valid_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
      "    \n",
      "    # 初始化模型、损失函数、优化器\n",
      "    model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "    \n",
      "    # 训练和验证\n",
      "    model, optimizer, fold_performance = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS)\n",
      "    \n",
      "    # 记录性能\n",
      "    for key in performance.keys():\n",
      "        performance[key].append(fold_performance[key])\n",
      "for key in performance.keys():\n",
      "    print(f'{key}: {np.mean(performance[key])} ± {np.std(performance[key])}')看起来似乎不错。有所提高。用上之前最终的模型看看from sklearn.model_selection import StratifiedKFold\n",
      "def k_fold(model, k, LEARNING_RATE):\n",
      "    print(model)\n",
      "    # 定义损失函数和优化器  \n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "\n",
      "    # 定义K折交叉验证\n",
      "    n_splits = k\n",
      "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
      "    performance = {'train_losses': [], 'valid_losses': [], 'train_acc': [], 'valid_acc': []}\n",
      "    # performance = {\n",
      "    #         'train_losses':train_losses,\n",
      "    #         'valid_losses': valid_losses,\n",
      "    #         'train_acc': train_accs,\n",
      "    #         'valid_acc':valid_accs\n",
      "    #     }\n",
      "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_preprocessed, labels)):\n",
      "        print(f'Fold {fold + 1}/{n_splits}')\n",
      "        \n",
      "        # 分割数据\n",
      "        X_train_fold, y_train_fold = X_preprocessed[train_idx], labels.iloc[train_idx]\n",
      "        X_val_fold, y_val_fold = X_preprocessed[val_idx], labels.iloc[val_idx]\n",
      "        \n",
      "\n",
      "        # 转换为PyTorch数据类型，确保先转换为密集格式\n",
      "        X_train_fold_dense = X_train_fold.toarray() if hasattr(X_train_fold, \"toarray\") else X_train_fold\n",
      "        X_val_fold_dense = X_val_fold.toarray() if hasattr(X_val_fold, \"toarray\") else X_val_fold\n",
      "\n",
      "        # 转换为PyTorch数据类型\n",
      "        X_train_fold_tensor = torch.tensor(X_train_fold_dense.astype(np.float32)).to(device)\n",
      "        y_train_fold_tensor = torch.tensor(y_train_fold.values.astype(np.int64)).to(device)\n",
      "        X_val_fold_tensor = torch.tensor(X_val_fold_dense.astype(np.float32)).to(device)\n",
      "        y_val_fold_tensor = torch.tensor(y_val_fold.values.astype(np.int64)).to(device)\n",
      "        \n",
      "        # DataLoader\n",
      "        train_dataset = TensorDataset(X_train_fold_tensor, y_train_fold_tensor)\n",
      "        val_dataset = TensorDataset(X_val_fold_tensor, y_val_fold_tensor)\n",
      "        train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
      "        valid_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
      "        \n",
      "        # 初始化模型、损失函数、优化器\n",
      "        model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "        criterion = nn.CrossEntropyLoss()\n",
      "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "        \n",
      "        # 训练和验证\n",
      "        model, optimizer, fold_performance = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS)\n",
      "        \n",
      "        # 记录性能\n",
      "        for key in performance.keys():\n",
      "            performance[key].append(fold_performance[key])\n",
      "    for key in performance.keys():\n",
      "        print(f'{key}: {np.mean(performance[key])} ± {np.std(performance[key])}')LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 10\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4,648,328,128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "model = SpaceshipTitanicModelDropOut(input_dim, output_dim, hidden_layers).to(device)\n",
      "k_fold(model,5,LEARNING_RATE)有明显改善。那么我就增加丢弃的概率，并且添加早停策略def training_loop_updateEarlyStop(model, criterion, optimizer, train_loader, valid_loader, epochs,patience=10,print_every=1):\n",
      "    train_losses = []\n",
      "    valid_losses = []\n",
      "    train_accs = []\n",
      "    valid_accs = []\n",
      "    best_valid_loss = float('inf')\n",
      "    epochs_no_improve = 0\n",
      "    best_model_state = None\n",
      "\n",
      "    for epoch in range(0, epochs):\n",
      "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer)\n",
      "        train_losses.append(train_loss)\n",
      "        with torch.no_grad():\n",
      "            model, valid_loss = validate(valid_loader, model, criterion)\n",
      "            valid_losses.append(valid_loss)\n",
      "        if valid_loss < best_valid_loss:\n",
      "            best_valid_loss = valid_loss\n",
      "            best_model_state = model.state_dict()\n",
      "            epochs_no_improve = 0\n",
      "        else:\n",
      "            epochs_no_improve += 1\n",
      "\n",
      "\n",
      "        if epochs_no_improve == patience:\n",
      "            print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
      "            break\n",
      "        if epoch % print_every == (print_every - 1):\n",
      "            train_acc = get_accuracy(model, train_loader,)\n",
      "            train_accs.append(train_acc)\n",
      "            valid_acc = get_accuracy(model, valid_loader)\n",
      "            valid_accs.append(valid_acc)\n",
      "            print(f'{datetime.now().time().replace(microsecond=0)} '\n",
      "                  f'Epoch: {epoch}\\t'\n",
      "                  f'Train loss: {train_loss:.4f}\\t'\n",
      "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
      "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
      "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
      "            # scheduler.step(valid_loss)\n",
      "    performance = {\n",
      "        'train_losses':train_losses,\n",
      "        'valid_losses': valid_losses,\n",
      "        'train_acc': train_accs,\n",
      "        'valid_acc':valid_accs\n",
      "    }\n",
      "    if best_model_state:\n",
      "        model.load_state_dict(best_model_state)\n",
      "    return model, optimizer, performancefrom sklearn.model_selection import StratifiedKFold\n",
      "def k_fold_earlyStop(model, k, LEARNING_RATE):\n",
      "    print(model)\n",
      "    # 定义损失函数和优化器  \n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "\n",
      "    # 定义K折交叉验证\n",
      "    n_splits = k\n",
      "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
      "    performance = {'train_losses': [], 'valid_losses': [], 'train_acc': [], 'valid_acc': []}\n",
      "    # performance = {\n",
      "    #         'train_losses':train_losses,\n",
      "    #         'valid_losses': valid_losses,\n",
      "    #         'train_acc': train_accs,\n",
      "    #         'valid_acc':valid_accs\n",
      "    #     }\n",
      "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_preprocessed, labels)):\n",
      "        print(f'Fold {fold + 1}/{n_splits}')\n",
      "        \n",
      "        # 分割数据\n",
      "        X_train_fold, y_train_fold = X_preprocessed[train_idx], labels.iloc[train_idx]\n",
      "        X_val_fold, y_val_fold = X_preprocessed[val_idx], labels.iloc[val_idx]\n",
      "        \n",
      "\n",
      "        # 转换为PyTorch数据类型，确保先转换为密集格式\n",
      "        X_train_fold_dense = X_train_fold.toarray() if hasattr(X_train_fold, \"toarray\") else X_train_fold\n",
      "        X_val_fold_dense = X_val_fold.toarray() if hasattr(X_val_fold, \"toarray\") else X_val_fold\n",
      "\n",
      "        # 转换为PyTorch数据类型\n",
      "        X_train_fold_tensor = torch.tensor(X_train_fold_dense.astype(np.float32)).to(device)\n",
      "        y_train_fold_tensor = torch.tensor(y_train_fold.values.astype(np.int64)).to(device)\n",
      "        X_val_fold_tensor = torch.tensor(X_val_fold_dense.astype(np.float32)).to(device)\n",
      "        y_val_fold_tensor = torch.tensor(y_val_fold.values.astype(np.int64)).to(device)\n",
      "        \n",
      "        # DataLoader\n",
      "        train_dataset = TensorDataset(X_train_fold_tensor, y_train_fold_tensor)\n",
      "        val_dataset = TensorDataset(X_val_fold_tensor, y_val_fold_tensor)\n",
      "        train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
      "        valid_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
      "        \n",
      "        # 初始化模型、损失函数、优化器\n",
      "        model = SpaceshipTitanicModel(input_dim, output_dim, hidden_layers).to(device)\n",
      "        criterion = nn.CrossEntropyLoss()\n",
      "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "        \n",
      "        # 训练和验证\n",
      "        model, optimizer, fold_performance = training_loop_updateEarlyStop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS)\n",
      "        \n",
      "        # 记录性能\n",
      "        for key in performance.keys():\n",
      "            performance[key].append(fold_performance[key])\n",
      "    for key in performance.keys():\n",
      "        print(f'{key}: {np.mean(performance[key])} ± {np.std(performance[key])}')LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 10\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4,648,328,128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "model = SpaceshipTitanicModelDropOut(input_dim, output_dim, hidden_layers,0.8).to(device)\n",
      "k_fold_earlyStop(model,5,LEARNING_RATE)可以看见，在训练准确率下降一点的情况下，验证集准确率上升了许多。所以现在可以对这个模型进行拟合。#model 1 不更新LR\n",
      "# 根据预处理后的数据维度初始化模型\n",
      "LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 100\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4,648,328,128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "\n",
      "model_fixLR = SpaceshipTitanicModelDropOut(input_dim, output_dim, hidden_layers,0.8).to(device)\n",
      "print(model_fixLR)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model_fixLR.parameters(), lr=LEARNING_RATE)\n",
      "\n",
      "model_fixLR, optimizer, performance = training_loop_updateEarlyStop(model_fixLR, criterion, optimizer,train_loader,valid_loader, N_EPOCHS)#model 2 更新LR\n",
      "# 根据预处理后的数据维度初始化模型\n",
      "from torch.optim import lr_scheduler\n",
      "LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 100\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [FEATURE_SIZE, FEATURE_SIZE//2, FEATURE_SIZE//4,648,328,128]  # 可以调整这些层的大小和数量\n",
      "\n",
      "\n",
      "model_NewLR = SpaceshipTitanicModelDropOut(input_dim, output_dim, hidden_layers,0.8).to(device)\n",
      "print(model_NewLR)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model_NewLR.parameters(), lr=LEARNING_RATE)\n",
      "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
      "model_NewLR, optimizer, performance = training_loop_updateEarlyStop(model_NewLR, criterion, optimizer,train_loader,valid_loader, N_EPOCHS)# 加载测试数据\n",
      "test_df = pd.read_csv('test.csv')\n",
      "\n",
      "# 应用相同的预处理步骤\n",
      "# 注意不要fit预处理器，而是直接转换，因为我们不想从测试数据中学习任何信息\n",
      "X_test_preprocessed = preprocessor.transform(test_df[features.columns])\n",
      "\n",
      "# 转换成密集的NumPy数组（如果需要的话）\n",
      "X_test_dense = X_test_preprocessed.toarray() if hasattr(X_test_preprocessed, \"toarray\") else X_test_preprocessed\n",
      "\n",
      "# 转换成PyTorch张量\n",
      "X_test_tensor = torch.tensor(X_test_dense.astype(np.float32)).to(device)\n",
      "model_fixLR.eval()  # 将模型设置为评估模式\n",
      "with torch.no_grad():  # 确保在此过程中不计算梯度\n",
      "    predictions = model_fixLR(X_test_tensor)\n",
      "    predictions = torch.sigmoid(predictions)  # 如果你的模型输出是logits，用sigmoid转换为概率\n",
      "    predicted_labels = predictions.argmax(dim=1)  # 获取概率最高的标签作为预测\n",
      "# 创建一个DataFrame以保存你的预测\n",
      "submission_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Transported': predicted_labels.cpu().numpy()})\n",
      "\n",
      "# 将预测转换回布尔值（如果需要的话）\n",
      "submission_df['Transported'] = submission_df['Transported'].map({0: False, 1: True})\n",
      "\n",
      "# 保存到csv文件，准备提交\n",
      "submission_df.to_csv('submission1.csv', index=False)\n",
      "model_NewLR.eval()  # 将模型设置为评估模式\n",
      "with torch.no_grad():  # 确保在此过程中不计算梯度\n",
      "    predictions = model_NewLR(X_test_tensor)\n",
      "    predictions = torch.sigmoid(predictions)  # 如果你的模型输出是logits，用sigmoid转换为概率\n",
      "    predicted_labels = predictions.argmax(dim=1)  # 获取概率最高的标签作为预测\n",
      "# 创建一个DataFrame以保存你的预测\n",
      "submission_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Transported': predicted_labels.cpu().numpy()})\n",
      "\n",
      "# 将预测转换回布尔值（如果需要的话）\n",
      "submission_df['Transported'] = submission_df['Transported'].map({0: False, 1: True})\n",
      "\n",
      "# 保存到csv文件，准备提交\n",
      "submission_df.to_csv('submission2.csv', index=False)\n",
      "尝试更新的策略：使用SDG并且在早停之前更改LRdef calcSocre(valid_acc, train_acc):\n",
      "    ratio = 0.0001\n",
      "    absValue = abs(valid_acc - train_acc)\n",
      "    return absValue\n",
      "def training_loop_updateSDG(model, criterion, optimizer, train_loader, valid_loader, epochs,patience=15,print_every=1):\n",
      "    copy_opt = optimizer\n",
      "    train_losses = []\n",
      "    valid_losses = []\n",
      "    train_accs = []\n",
      "    valid_accs = []\n",
      "    best_accs = float('-inf')\n",
      "    best_model_state = None\n",
      "    epochs_no_improve = 0\n",
      "    optimizer_switch_threshold = 7\n",
      "\n",
      "    for epoch in range(0, epochs):\n",
      "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer)\n",
      "        train_losses.append(train_loss)\n",
      "        with torch.no_grad():\n",
      "            model, valid_loss = validate(valid_loader, model, criterion)\n",
      "            valid_losses.append(valid_loss)\n",
      "        # if valid_accs*0.7 + train_accs*0.3 < best_accs:\n",
      "        #     best_accs = valid_accs*0.7 + train_accs*0.3 \n",
      "        #     best_model_state = model.state_dict()\n",
      "        #     print(\"Model update, ratio = {best_accs}\")\n",
      "        #     epochs_no_improve = 0\n",
      "        # else:\n",
      "        #     epochs_no_improve += 1\n",
      "        #     if epochs_no_improve == optimizer_switch_threshold:\n",
      "        #         print(\"Switching to SGD optimizer and reducing learning rate.\")\n",
      "        #         optimizer = sgd_optimizer\n",
      "\n",
      "        #     elif epochs_no_improve == patience:\n",
      "        #         print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
      "        #         break\n",
      "        if epoch % print_every == (print_every - 1):\n",
      "            train_acc = get_accuracy(model, train_loader,)\n",
      "            train_accs.append(train_acc)\n",
      "            valid_acc = get_accuracy(model, valid_loader)\n",
      "            valid_accs.append(valid_acc)\n",
      "            print(f'{datetime.now().time().replace(microsecond=0)} '\n",
      "                  f'Epoch: {epoch}\\t'\n",
      "                  f'Train loss: {train_loss:.4f}\\t'\n",
      "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
      "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
      "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
      "            if calcSocre(valid_acc,train_acc) < 0.1 and valid_acc > best_accs:\n",
      "                best_accs =  valid_acc\n",
      "                best_model_state = model.state_dict()\n",
      "                print(f\"Model update, ratio = {best_accs}\")\n",
      "                epochs_no_improve = 0\n",
      "                optimizer = copy_opt\n",
      "            else:\n",
      "                epochs_no_improve += 1\n",
      "                if epochs_no_improve == optimizer_switch_threshold:\n",
      "                    print(\"Switching to SGD optimizer and reducing learning rate.\")\n",
      "                    optimizer = sgd_optimizer\n",
      "                elif epochs_no_improve == patience:\n",
      "                    print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
      "                    break\n",
      "        scheduler.step(valid_loss)\n",
      "    performance = {\n",
      "        'train_losses':train_losses,\n",
      "        'valid_losses': valid_losses,\n",
      "        'train_acc': train_accs,\n",
      "        'valid_acc':valid_accs\n",
      "    }\n",
      "    if best_model_state:\n",
      "        model.load_state_dict(best_model_state)\n",
      "    return model, optimizer, performanceimport torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class SpaceshipTitanicModelDropOutBN(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, layers, dropout_rate=0.5):\n",
      "        super(SpaceshipTitanicModelDropOutBN, self).__init__()\n",
      "        self.layers = nn.ModuleList()\n",
      "        \n",
      "        # 添加第一个隐藏层，它接受输入数据\n",
      "        self.layers.append(nn.Linear(input_dim, layers[0]))\n",
      "        self.layers.append(nn.BatchNorm1d(layers[0]))  # 批归一化\n",
      "        self.layers.append(nn.ReLU())\n",
      "        self.layers.append(nn.Dropout(dropout_rate))\n",
      "        \n",
      "        # 动态添加更多的隐藏层\n",
      "        for i in range(1, len(layers)):\n",
      "            self.layers.append(nn.Linear(layers[i-1], layers[i]))\n",
      "            self.layers.append(nn.BatchNorm1d(layers[i]))  # 批归一化\n",
      "            self.layers.append(nn.ReLU())\n",
      "            self.layers.append(nn.Dropout(dropout_rate))\n",
      "        \n",
      "        # 输出层\n",
      "        self.layers.append(nn.Linear(layers[-1], output_dim))\n",
      "        \n",
      "    def forward(self, x):\n",
      "        for layer in self.layers:\n",
      "            x = layer(x) if isinstance(layer, nn.Linear) else x\n",
      "            x = layer(x)\n",
      "        return x\n",
      "#model 3 更新梯度下降器\n",
      "# 根据预处理后的数据维度初始化模型\n",
      "import torch.optim as optim\n",
      "LEARNING_RATE = 0.001\n",
      "N_EPOCHS = 10000\n",
      "FEATURE_SIZE = 6576\n",
      "input_dim = X_train.shape[1]\n",
      "output_dim = 2  # 二分类问题\n",
      "hidden_layers = [648,1280]  # 可以调整这些层的大小和数量\n",
      "\n",
      "\n",
      "model_ERChange = SpaceshipTitanicModelDropOut(input_dim, output_dim, hidden_layers,0.95).to(device)\n",
      "print(model_ERChange)\n",
      "# 定义损失函数和优化器\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model_ERChange.parameters(), lr=LEARNING_RATE)\n",
      "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)\n",
      "sgd_optimizer = optim.SGD(model_ERChange.parameters(), lr=0.0001, momentum=0.9)\n",
      "\n",
      "model_ERChange, optimizer, performance = training_loop_updateSDG(model_ERChange, criterion, optimizer,train_loader,valid_loader, N_EPOCHS)# 加载测试数据\n",
      "test_df = pd.read_csv('test.csv')\n",
      "\n",
      "# 应用相同的预处理步骤\n",
      "# 注意不要fit预处理器，而是直接转换，因为我们不想从测试数据中学习任何信息\n",
      "X_test_preprocessed = preprocessor.transform(test_df[features.columns])\n",
      "\n",
      "# 转换成密集的NumPy数组（如果需要的话）\n",
      "X_test_dense = X_test_preprocessed.toarray() if hasattr(X_test_preprocessed, \"toarray\") else X_test_preprocessed\n",
      "\n",
      "# 转换成PyTorch张量\n",
      "X_test_tensor = torch.tensor(X_test_dense.astype(np.float32)).to(device)\n",
      "model_ERChange.eval()  # 将模型设置为评估模式\n",
      "with torch.no_grad():  # 确保在此过程中不计算梯度\n",
      "    predictions = model_ERChange(X_test_tensor)\n",
      "    predictions = torch.sigmoid(predictions)  # 如果你的模型输出是logits，用sigmoid转换为概率\n",
      "    predicted_labels = predictions.argmax(dim=1)  # 获取概率最高的标签作为预测\n",
      "# 创建一个DataFrame以保存你的预测\n",
      "submission_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Transported': predicted_labels.cpu().numpy()})\n",
      "\n",
      "# 将预测转换回布尔值（如果需要的话）\n",
      "submission_df['Transported'] = submission_df['Transported'].map({0: False, 1: True})\n",
      "\n",
      "# 保存到csv文件，准备提交\n",
      "submission_df.to_csv('submission2.csv', index=False)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   1%|          | 2/283 [00:51<2:19:28, 29.78s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from scipy.stats import zscore\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
      "from fancyimpute import IterativeImputer\n",
      "dataset_df = pd.read_csv(\"train.csv\")\n",
      "print(\"Full train dataset shape is {}\".format(dataset_df.shape))\n",
      "dataset_df.head(2)dataset_df.describe()#Prepare the dataset\n",
      "print(dataset_df.shape)\n",
      "dataset_df = dataset_df.drop(['PassengerId', 'Name'], axis=1)\n",
      "print(dataset_df.shape)\n",
      "dataset_df.head(2)dataset_df.isnull().sum().sort_values(ascending=False)# Imputing null values\n",
      "# def clear_null(data):\n",
      "#     for i in data.columns:\n",
      "#         if data[i].dtypes in ['object','bool']:\n",
      "#             data[i].fillna(data[i].mode()[0],inplace=True)\n",
      "#         else :\n",
      "#             data[i].fillna(data[i].mean(),inplace=True)\n",
      "#     return data\n",
      "#one hot encoding\n",
      "def one_hot(data):\n",
      "    data_temp = data\n",
      "    data_temp['Num'] = pd.to_numeric(data_temp['Num'], errors='coerce').astype('Int64')\n",
      "    data_encoded = data_temp\n",
      "\n",
      "    #print(data_encoded.shape)\n",
      "\n",
      "    # Identifying categorical variables (excluding 'Cabin') and boolean variables for one-hot encoding\n",
      "    categorical_vars = data_temp.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
      "    #categorical_vars.drop('Cabin')  # Exclude 'Cabin'\n",
      "\n",
      "    # Applying one-hot encoding\n",
      "    data_encoded = pd.get_dummies(data_temp, columns=categorical_vars, drop_first=True)\n",
      "    print(data_encoded.shape)\n",
      "    return data_encoded\n",
      "\n",
      "def pre_data(data):\n",
      "    data_temp = data\n",
      "    # 假设 data_temp 是你的 DataFrame\n",
      "    # 检查 'Cabin' 是否为空或者是 NaN，并根据情况分割或者赋值为空值\n",
      "    mask = data_temp['Cabin'].notna()  # 创建一个掩码来标记非空的 'Cabin' 值\n",
      "\n",
      "    # 对于非空的 'Cabin'，进行分割；否则赋值为 NaN\n",
      "    data_temp['Deck'] = np.where(mask, data_temp['Cabin'].str.split('/', expand=True)[0], np.nan)\n",
      "    data_temp['Num'] = np.where(mask, data_temp['Cabin'].str.split('/', expand=True)[1], np.nan)\n",
      "    data_temp['Side'] = np.where(mask, data_temp['Cabin'].str.split('/', expand=True)[2], np.nan)\n",
      "    # data_temp['Deck'], data_temp['Num'], data_temp['Side'] = data_temp['Cabin'].str.split('/', 2).str\n",
      "    data_temp.drop('Cabin', axis=1,inplace=True)\n",
      "    return data_temp\n",
      "\n",
      "def clear_null(data):\n",
      "    data = data.interpolate() \n",
      "    data = pre_data(data)\n",
      "    data = one_hot(data)\n",
      "    #data_no_null = data.dropna()\n",
      "\n",
      "    # 使用IterativeImputer进行多重插补\n",
      "    imputer = IterativeImputer(sample_posterior = True,initial_strategy = 'most_frequent')\n",
      "\n",
      "    # 假定df是你需要处理的DataFrame\n",
      "    imputed_data = imputer.fit_transform(data)\n",
      "\n",
      "    # 将插补后的数据转换回DataFrame\n",
      "    data = pd.DataFrame(imputed_data, columns=data.columns)\n",
      "   \n",
      "    # for i in data.columns:\n",
      "    #     if data[i].dtypes in ['object','bool']:\n",
      "    #         data[i].fillna(data[i].mode()[0],inplace=True)\n",
      "    #     else :\n",
      "    #         data[i].fillna(data[i].mean(),inplace=True)\n",
      "    return data\n",
      "\n",
      "dataset_test = pd.read_csv(\"test.csv\")\n",
      "print(dataset_test.shape)\n",
      "id = dataset_test[\"PassengerId\"]\n",
      "dataset_test.drop(['PassengerId', 'Name'], axis=1,inplace=True)\n",
      "\n",
      "label = dataset_df['Transported']\n",
      "dataset_df.drop(['Transported'], axis=1,inplace=True)\n",
      "print(dataset_df.shape)\n",
      "print(dataset_test.shape)\n",
      "combined_df = pd.concat([dataset_df, dataset_test], axis=0).reset_index(drop=True)\n",
      "\n",
      "print(dataset_df.shape)\n",
      "\n",
      "combined_df_cleaned = clear_null(combined_df)\n",
      "\n",
      "original_dataset_df_rows = len(dataset_df)\n",
      "dataset_df_cleaned = combined_df_cleaned.iloc[:original_dataset_df_rows]\n",
      "dataset_test_cleaned = combined_df_cleaned.iloc[original_dataset_df_rows:]\n",
      "\n",
      "dataset_df = dataset_df_cleaned\n",
      "dataset_test = dataset_test_cleaned\n",
      "dataset_df['Transported'] = label\n",
      "dataset_df['Transported'] = dataset_df['Transported'].astype(int)\n",
      "\n",
      "#dataset_df.isnull().sum().sort_values(ascending=False)\n",
      "print(dataset_test.shape)# def plot_df(dataset_df):   \n",
      "#     n_cols = 2\n",
      "#     categorical_cols = dataset_df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
      "#     n_rows = int(np.ceil(len(categorical_cols) / n_cols))\n",
      "#     # Plotting the distributions for categorical columns using bar charts\n",
      "#     fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(10, 10))\n",
      "#     for i, col in enumerate(categorical_cols):\n",
      "#         row = i // n_cols\n",
      "#         col_idx = i % n_cols\n",
      "        \n",
      "#         # sns.countplot(..., ax=axs[row, col_idx]) 当 n_rows 或 n_cols 为1时，axs可能不是二维的\n",
      "#         if n_rows == 1:\n",
      "#             ax = axs[col_idx]\n",
      "#         elif n_cols == 1:\n",
      "#             ax = axs[row]\n",
      "#         else:\n",
      "#             ax = axs[row, col_idx]\n",
      "        \n",
      "#         sns.countplot(x=col, data=dataset_df, ax=ax, palette='coolwarm')\n",
      "#         ax.set_title(f'Distribution of {col}', fontsize=15)\n",
      "#         ax.set_xlabel('')\n",
      "#         ax.set_ylabel('')\n",
      "#         ax.tick_params(axis='x', rotation=45)\n",
      "\n",
      "#     #hidden blank charts\n",
      "#     for j in range(i + 1, n_rows * n_cols):\n",
      "#         fig.delaxes(axs.flatten()[j])\n",
      "\n",
      "#     fig.tight_layout(pad=3.0)\n",
      "#     plt.show()\n",
      "\n",
      "# plot_df(dataset_df)# # Calculate Z-scores for numeric columns\n",
      "# numeric_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
      "# z_scores = dataset_df[numeric_cols].apply(zscore, nan_policy='omit')\n",
      "\n",
      "# # Identify rows where any Z-score is greater than 3 or less than -3 (outliers)\n",
      "# outliers = (z_scores.abs() > 3).any(axis=1)\n",
      "\n",
      "# # Count the number of outliers\n",
      "# num_outliers = outliers.sum()\n",
      "\n",
      "# # Remove outliers from the dataset\n",
      "# df_clean = dataset_df[~outliers]\n",
      "\n",
      "# # Number of rows before and after outlier removal\n",
      "# rows_before = dataset_df.shape[0]\n",
      "# rows_after = df_clean.shape[0]\n",
      "\n",
      "# num_outliers, rows_before, rows_after#dataset_df = df_clean\n",
      "\n",
      "#plot_df(dataset_df)#data_encoded.drop('Cabin', axis=1)\n",
      "data_encoded = dataset_df\n",
      "data_encoded.head()dataset_df = data_encoded.copy()\n",
      "data_encoded.drop(['Transported'], axis=1,inplace=True)\n",
      "\n",
      "#x_train,x_test,y_train,y_test=train_test_split(data_encoded,dataset_df['Transported'],test_size=.2,shuffle=True,random_state=42)\n",
      "X = data_encoded\n",
      "y = dataset_df['Transported']\n",
      "X.head(5)from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score, classification_report\n",
      "import torch\n",
      "from torch.utils.data import DataLoader, TensorDataset\n",
      "from torch import nn\n",
      "import torch.optim as optim\n",
      "\n",
      "print(type(dataset_test))\n",
      "# 先将DataFrame和Series转换为NumPy数组\n",
      "dataset_test = dataset_test.to_numpy().astype(np.float32)  # 确保数据类型为float32，以兼容PyTorch\n",
      "# 先将DataFrame和Series转换为NumPy数组\n",
      "X_np = X.to_numpy().astype(np.float32)  # 确保数据类型为float32，以兼容PyTorch\n",
      "y_np = y.to_numpy().astype(np.float32).reshape(-1, 1)  # 对于y，确保它是2D数组，适合后续处理\n",
      "\n",
      "# 然后将NumPy数组转换为PyTorch的Tensor\n",
      "X_tensor = torch.from_numpy(X_np)\n",
      "y_tensor = torch.from_numpy(y_np)\n",
      "\n",
      "X, y = torch.tensor(X_tensor), torch.tensor(y_tensor)\n",
      "\n",
      "# 将数据封装成 DataLoader，方便分批训练\n",
      "batch_size = 8693\n",
      "dataset = TensorDataset(X, y)\n",
      "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
      "\n",
      "# 定义模型\n",
      "class NeuralNetwork(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(NeuralNetwork, self).__init__()\n",
      "        self.flatten = nn.Flatten()\n",
      "        self.linear_relu_stack = nn.Sequential(\n",
      "            nn.Linear(X.shape[1], 64),\n",
      "            nn.ReLU(),\n",
      "            nn.Linear(64, 64),\n",
      "            nn.ReLU(),\n",
      "            # nn.Linear(64, 128),\n",
      "            # nn.ReLU(),\n",
      "            # nn.Linear(128, 64),\n",
      "            # nn.ReLU(),\n",
      "            nn.Linear(64, 1),\n",
      "            nn.Sigmoid(),\n",
      "        )\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.flatten(x)\n",
      "        logits = self.linear_relu_stack(x)\n",
      "        return logits\n",
      "\n",
      "model = NeuralNetwork()\n",
      "\n",
      "# 定义损失函数和优化器\n",
      "loss_fn = nn.BCELoss()\n",
      "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
      "\n",
      "# 训练过程\n",
      "def train(dataloader, model, loss_fn, optimizer):\n",
      "    size = len(dataloader.dataset)\n",
      "    model.train()\n",
      "    for batch, (X, y) in enumerate(dataloader):\n",
      "        # 计算预测误差\n",
      "        pred = model(X)\n",
      "        loss = loss_fn(pred, y)  # 确保y的形状与预测值匹配\n",
      "\n",
      "        # 反向传播\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch % 100 == 0:\n",
      "            loss, current = loss.item(), batch * len(X)\n",
      "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
      "\n",
      "# 训练模型\n",
      "epochs = 30\n",
      "for t in range(epochs):\n",
      "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
      "    train(dataloader, model, loss_fn, optimizer)\n",
      "print(\"Done!\")\n",
      "\n",
      "# # 评估模型性能\n",
      "# loss, accuracy = model.evaluate(x_test, y_test)\n",
      "# print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
      "\n",
      "\n",
      "# # Predicting on the validation set\n",
      "# y_pred = model.predict(x_test)\n",
      "\n",
      "# # Evaluating the model\n",
      "# print('classification report for Random Forest Classifier \\n',classification_report(y_pred,y_test))print(type(dataset_test))\n",
      "\n",
      "\n",
      "# 然后将NumPy数组转换为PyTorch的Tensor\n",
      "dataset_test = torch.from_numpy(dataset_test)\n",
      "\n",
      "dataset_test= torch.tensor(dataset_test)\n",
      "\n",
      "model.eval()  # 将模型设置为评估模式\n",
      "with torch.no_grad():  # 关闭梯度计算，因为在预测时我们不需要计算梯度\n",
      "    predictions = model(dataset_test)  # 直接调用模型来获取预测结果\n",
      "\n",
      "# 如果你的输出层使用了Sigmoid激活函数并且是二分类问题，可能需要进一步处理这些预测结果\n",
      "# 例如，将输出概率转换为类别标签\n",
      "y_pred = (predictions.squeeze() > 0.5).bool() # 假设阈值为0.5，squeeze去除单维度条目results = pd.DataFrame({'PassengerId': id, 'Transported': y_pred})\n",
      "\n",
      "results.to_csv(\"prediction_results.csv\", index=False)\n",
      "\n",
      "print(\"Done!\")\n"
     ]
    }
   ],
   "source": [
    "# 使用ollama生成目录\n",
    "import ollama\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def generate_summary_with_ollama(text):\n",
    "    try:\n",
    "        # 使用 Ollama 模型生成内容摘要\n",
    "        response = ollama.chat(model=\"llama3.2-vision:latest\", messages=[{\"role\": \"user\", \"content\": text}])\n",
    "        return response['text']\n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "# 逐文件处理并保存到txt文件\n",
    "def process_files_and_save_to_txt(folder_path, output_txt_file):\n",
    "    # 获取所有文件\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    # 打开文件进行写入\n",
    "    with open(output_txt_file, 'w', encoding='utf-8') as f:\n",
    "        # 使用tqdm显示进度条\n",
    "        for file_path in tqdm(file_paths, desc=\"Processing Files\", unit=\"file\"):\n",
    "            file_extension = file_path.split('.')[-1].lower()\n",
    "            content = \"\"\n",
    "\n",
    "            if file_extension == 'pdf':\n",
    "                content = extract_pdf_text(file_path)\n",
    "            elif file_extension == 'ipynb':\n",
    "                content = extract_ipynb_text(file_path)\n",
    "            # elif file_extension == 'csv':\n",
    "            #     content = extract_csv_text(file_path)\n",
    "            # elif file_extension == 'txt':\n",
    "            #     content = extract_txt_text(file_path)\n",
    "            elif file_extension == 'py':\n",
    "                content = extract_py_text(file_path)\n",
    "            else:\n",
    "                content = \"No text extracted\"\n",
    "            \n",
    "            print(content)\n",
    "\n",
    "            # 生成摘要并写入文件\n",
    "            summary = generate_summary_with_ollama(content)\n",
    "            f.write(f\"File: {file_path}\\nSummary: {summary}\\n\\n\")\n",
    "            f.flush()\n",
    "\n",
    "# 示例：使用函数处理文件夹\n",
    "folder_path = 'C:/Users/Dingz/Desktop/273'  # 替换为你的文件夹路径\n",
    "output_txt_file = os.path.join(folder_path, \"file_directory_summary.txt\")  # 保存目录摘要的文件路径\n",
    "process_files_and_save_to_txt(folder_path, output_txt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_txt_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_txt_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# 使用tqdm显示进度条\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Dingz/Desktop/273/file_directory_summary.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Files\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      4\u001b[0m         file_extension \u001b[38;5;241m=\u001b[39m file_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_txt_file' is not defined"
     ]
    }
   ],
   "source": [
    "with open('C:/Users/Dingz/Desktop/273/file_directory_summary.txt', 'w', encoding='utf-8') as f:\n",
    "    # 使用tqdm显示进度条\n",
    "    for file_path in tqdm('C:/Users/Dingz/Desktop/273/file_directory_summary.txt', desc=\"Processing Files\", unit=\"file\"):\n",
    "        file_extension = file_path.split('.')[-1].lower()\n",
    "        content = \"\"\n",
    "\n",
    "        # 生成摘要并写入文件\n",
    "        summary = generate_summary_with_ollama(content)\n",
    "        f.write(f\"File: {file_path}\\nSummary: {summary}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
